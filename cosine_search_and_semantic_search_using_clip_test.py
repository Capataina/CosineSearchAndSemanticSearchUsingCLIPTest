# -*- coding: utf-8 -*-
"""Cosine Search and Semantic Search using CLIP Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uuSFgjXZV3-sEFtQ0VhIBXEcJopOrvI7
"""

# Install dependencies
!pip install transformers torch pillow numpy matplotlib

# Imports
import torch
import numpy as np
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import matplotlib.pyplot as plt
from pathlib import Path
import time
from typing import List, Tuple
import json

# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

print(f"Using device: {device}")
print(f"Model loaded: {model.config.model_type}")

# Setup directories
image_dir = Path("/content/reference_images")
image_dir.mkdir(exist_ok=True)

# Upload your images to /content/reference_images
# Or download them with: !wget <your_url> -P /content/reference_images

# Get all image paths
image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
image_paths = [
    p for p in image_dir.rglob("*")
    if p.suffix.lower() in image_extensions
]
print(f"Found {len(image_paths)} images")

# Encode images in batches
def encode_images_batch(image_paths: List[Path], batch_size: int = 32) -> Tuple[np.ndarray, List[Path]]:
    """Encode all images and return embeddings + valid paths"""
    all_embeddings = []
    valid_paths = []

    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i + batch_size]
        batch_images = []
        batch_valid_paths = []

        # Load images
        for path in batch_paths:
            try:
                img = Image.open(path).convert("RGB")
                batch_images.append(img)
                batch_valid_paths.append(path)
            except Exception as e:
                print(f"Failed to load {path}: {e}")
                continue

        if not batch_images:
            continue

        # Process and encode
        inputs = processor(images=batch_images, return_tensors="pt", padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            embeddings = model.get_image_features(**inputs)
            embeddings = embeddings.cpu().numpy()

        all_embeddings.append(embeddings)
        valid_paths.extend(batch_valid_paths)

        print(f"Encoded {len(valid_paths)}/{len(image_paths)} images", end='\r')

    print()  # New line after progress
    return np.vstack(all_embeddings), valid_paths

# Encode all images
start_time = time.time()
embeddings, valid_paths = encode_images_batch(image_paths, batch_size=32)
encoding_time = time.time() - start_time

print(f"Encoding completed in {encoding_time:.2f} seconds")
print(f"Embeddings shape: {embeddings.shape}")
print(f"Average time per image: {encoding_time/len(valid_paths):.3f} seconds")

# Normalize embeddings for cosine similarity
embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

# Similarity search
def find_similar_images(query_idx: int, embeddings: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
    """Find top-k most similar images to query"""
    query_embedding = embeddings[query_idx:query_idx+1]

    # Cosine similarity via dot product (embeddings are normalized)
    similarities = np.dot(embeddings, query_embedding.T).squeeze()

    # Get top-k indices (excluding the query itself if it's in the results)
    top_indices = np.argsort(similarities)[::-1][:top_k]
    top_scores = similarities[top_indices]

    return top_indices, top_scores

def display_search_results(query_idx: int, similar_indices: np.ndarray,
                          scores: np.ndarray, paths: List[Path],
                          num_display: int = 10):
    """Display query image and its top matches"""
    # Add 1 to num_display to account for query image
    total_display = min(num_display + 1, len(similar_indices) + 1)

    # Create grid - adjust rows/cols based on total images
    cols = 5
    rows = (total_display + cols - 1) // cols  # Ceiling division

    fig, axes = plt.subplots(rows, cols, figsize=(20, 4*rows))
    if rows == 1:
        axes = axes.reshape(1, -1)  # Ensure 2D array
    axes = axes.flatten()

    # Display query image first
    query_img = Image.open(paths[query_idx])
    axes[0].imshow(query_img)
    axes[0].set_title(f"QUERY IMAGE\n{paths[query_idx].name}", fontsize=10, fontweight='bold', color='red')
    axes[0].axis('off')

    # Display similar images
    for i in range(min(num_display, len(similar_indices))):
        idx = similar_indices[i]
        img = Image.open(paths[idx])
        axes[i+1].imshow(img)
        axes[i+1].set_title(f"Rank {i+1}\nScore: {scores[i]:.3f}", fontsize=10)
        axes[i+1].axis('off')

    # Hide any unused subplots
    for i in range(total_display, len(axes)):
        axes[i].axis('off')

    plt.tight_layout()
    plt.show()

# Test similarity search on a few random images
num_tests = 3
test_indices = np.random.choice(len(valid_paths), size=num_tests, replace=False)

for query_idx in test_indices:
    print(f"\nQuery: {valid_paths[query_idx].name}")

    search_start = time.time()
    similar_indices, scores = find_similar_images(query_idx, embeddings_normalized, top_k=10)
    search_time = time.time() - search_start

    print(f"Search completed in {search_time*1000:.2f} ms")
    print(f"Top 5 similar images:")
    for i, (idx, score) in enumerate(zip(similar_indices[:5], scores[:5])):
        print(f"  {i+1}. {valid_paths[idx].name} (score: {score:.3f})")

    display_search_results(query_idx, similar_indices, scores, valid_paths)

# Save embeddings for reuse
np.save("/content/embeddings.npy", embeddings_normalized)
with open("/content/image_paths.json", "w") as f:
    json.dump([str(p) for p in valid_paths], f)

print("\nEmbeddings saved to /content/embeddings.npy")
print("Image paths saved to /content/image_paths.json")

def diverse_similarity_search(query_idx: int, embeddings: np.ndarray,
                               num_results: int = 12) -> Tuple[np.ndarray, np.ndarray]:
    """
    Find similar images with diversity sampling.

    Bucketing strategy:
    - Top 1-50: exact matches (same pose/angle) -> sample 1-2 images
    - 51-125: similar but varied -> sample 5-6 images (bulk of results)
    - 126-200: loosely related -> sample 2-3 images
    """
    # Get top 200 similar images
    query_embedding = embeddings[query_idx:query_idx+1]
    similarities = np.dot(embeddings, query_embedding.T).squeeze()
    top_200_indices = np.argsort(similarities)[::-1][:200]
    top_200_scores = similarities[top_200_indices]

    # Define buckets (excluding query image at index 0)
    bucket_1 = top_200_indices[1:150]      # indices 1-49 (skip query)
    bucket_2 = top_200_indices[150:300]    # indices 50-124
    bucket_3 = top_200_indices[300:500]   # indices 125-199

    # Sample from each bucket
    num_from_bucket_1 = min(2, len(bucket_1))
    num_from_bucket_2 = min(6, len(bucket_2))
    num_from_bucket_3 = num_results - num_from_bucket_1 - num_from_bucket_2

    selected_indices = []

    # Sample from bucket 1 (exact matches) - 1-2 images
    if len(bucket_1) > 0:
        selected_1 = np.random.choice(bucket_1, size=num_from_bucket_1, replace=False)
        selected_indices.extend(selected_1)

    # Sample from bucket 2 (similar but varied) - 5-6 images
    if len(bucket_2) > 0:
        selected_2 = np.random.choice(bucket_2, size=num_from_bucket_2, replace=False)
        selected_indices.extend(selected_2)

    # Sample from bucket 3 (loosely related) - remaining slots
    if len(bucket_3) > 0 and num_from_bucket_3 > 0:
        selected_3 = np.random.choice(bucket_3, size=min(num_from_bucket_3, len(bucket_3)), replace=False)
        selected_indices.extend(selected_3)

    selected_indices = np.array(selected_indices)
    selected_scores = similarities[selected_indices]

    # Sort by score for display (highest to lowest)
    sort_order = np.argsort(selected_scores)[::-1]
    selected_indices = selected_indices[sort_order]
    selected_scores = selected_scores[sort_order]

    return selected_indices, selected_scores

# Test the diverse search
test_indices = np.random.choice(len(valid_paths), size=3, replace=False)

for query_idx in test_indices:
    print(f"\nQuery: {valid_paths[query_idx].name}")

    # Compare original vs diverse search
    print("\n--- DIVERSE SEARCH ---")
    diverse_indices, diverse_scores = diverse_similarity_search(query_idx, embeddings_normalized, num_results=9)

    print(f"Top 9 diverse results:")
    for i, (idx, score) in enumerate(zip(diverse_indices, diverse_scores)):
        print(f"  {i+1}. {valid_paths[idx].name} (score: {score:.3f})")

    display_search_results(query_idx, diverse_indices, diverse_scores, valid_paths, num_display=9)

def text_search(text_query: str, embeddings: np.ndarray, paths: List[Path], top_k: int = 20):
    """Search images using text description"""
    # Encode text query
    inputs = processor(text=[text_query], return_tensors="pt", padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        text_embedding = model.get_text_features(**inputs)
        text_embedding = text_embedding.cpu().numpy()

    # Normalize
    text_embedding = text_embedding / np.linalg.norm(text_embedding)

    # Compute similarities
    similarities = np.dot(embeddings, text_embedding.T).squeeze()

    # Get top-k
    top_indices = np.argsort(similarities)[::-1][:top_k]
    top_scores = similarities[top_indices]

    return top_indices, top_scores

# Test it
queries = [
    "a person kneeling with tattoos facing the camera",
    "kneeling person with tattoos",
    "kneeling, tattoos, facing camera",
    "tattoo kneeling face"
]

for query in queries:
    print(f"\n{'='*60}")
    print(f"Text query: '{query}'")
    print(f"{'='*60}")

    indices, scores = text_search(query, embeddings_normalized, valid_paths, top_k=9)

    print(f"Top 3 results:")
    for i in range(3):
        print(f"  {i+1}. {valid_paths[indices[i]].name} (score: {scores[i]:.3f})")

    # Visualize
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    axes = axes.flatten()

    for i in range(9):
        img = Image.open(valid_paths[indices[i]])
        axes[i].imshow(img)
        axes[i].set_title(f"Rank {i+1}\nScore: {scores[i]:.3f}", fontsize=10)
        axes[i].axis('off')

    axes[9].axis('off')  # Hide last subplot
    plt.suptitle(f"Text Query: '{query}'", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

# Performance summary
print(f"\n{'='*50}")
print(f"PERFORMANCE SUMMARY")
print(f"{'='*50}")
print(f"Total images: {len(valid_paths)}")
print(f"Embedding dimension: {embeddings.shape[1]}")
print(f"Total encoding time: {encoding_time:.2f} seconds")
print(f"Average time per image: {encoding_time/len(valid_paths):.3f} seconds")
print(f"Memory footprint: {embeddings.nbytes / 1024 / 1024:.2f} MB")
print(f"Average search time: ~{search_time*1000:.2f} ms")